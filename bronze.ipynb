{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0497f57-4b5c-4ed4-ae06-d33a1763abdc",
   "metadata": {},
   "source": [
    " # Part 1 - Bronze layer\n",
    "- Raw data ingestion (No parsing or transformation)\n",
    "- Adding technical metadata ie columns [1. _ingestion_timestamp,2. _source,3. _status]\n",
    "- Error handling (missing file, unreadable rows, empty lines)\n",
    "- Some Data Quality Metrics (line count, empty line count)\n",
    "- Save to data/ukprop_bronze.parquet (the optimal file type is checked in the silver layer, we went for the standard here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6b50-e009-4431-94a1-b500465115b5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878e7dd6-ed86-4b44-91a9-11fc22d19e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is available!\n",
      "✓ SparkSession created successfully!\n",
      "Spark Version: 3.5.0\n",
      "Scala version: 2.12.18\n",
      "Application Name: DAT535-Project\n",
      "Master: local[*]\n",
      "Default Parallelism: 4\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import findspark\n",
    "import csv\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import * # isnan, count, desc\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql import Row\n",
    "\n",
    "    pyspark_available = True\n",
    "    print(\"PySpark is available!\")\n",
    "except ImportError:\n",
    "    print(\"PySpark not found. Please install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "if not pyspark_available:\n",
    "    raise ImportError(\"Cannot proceed without PySpark. Please install PySpark first.\")\n",
    "\n",
    "# Create SparkSession with custom configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DAT535-Project\") \\\n",
    "     .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "     .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "     .config(\"spark.driver.memory\", \"2g\") \\\n",
    "     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✓ SparkSession created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Scala version: {spark.sparkContext._gateway.jvm.scala.util.Properties.versionNumberString()}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257e8d2-e87e-4cf3-9f3a-6f1eab2f1750",
   "metadata": {},
   "source": [
    "### RDD -> DataFrame hybrid approach\n",
    "+ Map/Reduce fundamentals as requested by our God lord and savior (Tomasz)\n",
    "+ Allows per line error handling without failing the pipeline (try/except), useful to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e904e2-05c6-4e4b-9d83-cfa8bc289f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bronze Layer: Raw Data Ingestion ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/24 12:07:02 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 54 (TID 687): Attempting to kill Python Worker\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------+\n",
      "|_ingestion_timestamp|           _raw_data|             _source|_status|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "|1.7639860183155077E9|A newly built lea...|data/ukprop_unstr...|  valid|\n",
      "|1.7639860183155622E9|A newly built fre...|data/ukprop_unstr...|  valid|\n",
      "|1.7639860183155687E9|This newly built ...|data/ukprop_unstr...|  valid|\n",
      "|1.7639860183155925E9|An newly built fr...|data/ukprop_unstr...|  valid|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Execution time: 4.44 seconds\n",
      "Counting data for the data quality metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:======================================================> (71 + 2) / 73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Metrics:\n",
      "total_records: 5732838\n",
      "valid_records: 5732838\n",
      "empty_records: 0\n",
      "error_records: 0\n",
      "success_rates: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=== Bronze Layer: Raw Data Ingestion ===\")\n",
    "\n",
    "source = \"data/ukprop_unstructured.txt\"\n",
    "\n",
    "# Parse Line and handle errors (Bronze layer pattern)\n",
    "def parse_line_safe(line):\n",
    "    try:\n",
    "        return {\n",
    "            \"_raw_data\": line,\n",
    "            \"_ingestion_timestamp\": time.time(),\n",
    "            \"_source\": source,\n",
    "            \"_status\": \"valid\"\n",
    "        }\n",
    "    except Exception as error:\n",
    "        return {\n",
    "            \"_raw_data\": line,\n",
    "            \"_ingestion_timestamp\": time.time(),\n",
    "            \"_source\": source,\n",
    "            \"_status\": \"parse_error\"\n",
    "        }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create RDD from raw text\n",
    "raw_rdd = spark.sparkContext.textFile(source)\n",
    "bronze_rdd = raw_rdd.map(parse_line_safe)\n",
    "\n",
    "df_bronze = spark.createDataFrame(bronze_rdd)\n",
    "df_bronze.show(n=4, truncate=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Counting data for the data quality metrics\")\n",
    "total_records = df_bronze.count()\n",
    "empty_records = df_bronze.filter(trim(col(\"_raw_data\")) == \"\").count()\n",
    "valid_records = df_bronze.filter(col(\"_status\") == \"valid\").count()\n",
    "error_records = df_bronze.filter(col(\"_status\") == \"parse_error\").count()\n",
    "success_rates = (valid_records/total_records)*100\n",
    "\n",
    "print(\"\\nData Quality Metrics:\")\n",
    "print(f\"total_records: {total_records}\")\n",
    "print(f\"valid_records: {valid_records}\")\n",
    "print(f\"empty_records: {empty_records}\")\n",
    "print(f\"error_records: {error_records}\")\n",
    "print(f\"success_rates: {success_rates:.1f}%\")\n",
    "\n",
    "# not needed as per today. Errors were not really present here so. Therefore we added no more error handling\n",
    "# print(\"\\nError type breakdown:\") \n",
    "# bronze_df.filter(col(\"_status\") != \"valid\").groupBy(\"_status\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bc9a2-dd4a-4e3a-9b64-038d7bbe50b6",
   "metadata": {},
   "source": [
    "### (Alternative) Dataframe approach\n",
    "+ Faster (cayalyst optimizer)\n",
    "+ Cleaner syntax\n",
    "+ No manual parsing in .py\n",
    "+ Easy metadata columns (current_timestamp, lit)\n",
    "+ -Crashes if one fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f250be-5501-441c-b6a3-6d6b045debc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------+\n",
      "|           _raw_data|_ingestion_timestamp|             _source|_status|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "|A newly built lea...|2025-11-24 12:06:...|data/ukprop_unstr...|  valid|\n",
      "|A newly built fre...|2025-11-24 12:06:...|data/ukprop_unstr...|  valid|\n",
      "|This newly built ...|2025-11-24 12:06:...|data/ukprop_unstr...|  valid|\n",
      "|An newly built fr...|2025-11-24 12:06:...|data/ukprop_unstr...|  valid|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Execution time: 0.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=========================================>              (14 + 4) / 19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Metrics:\n",
      "total_records: 5732838\n",
      "valid_records: 5732838\n",
      "empty_records: 0\n",
      "error_records: 0\n",
      "success_rates: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Read raw text\n",
    "source = \"data/ukprop_unstructured.txt\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_bronze_raw = spark.read.text(source).withColumnRenamed(\"value\", \"_raw_data\")\n",
    "\n",
    "# 2. Add ingestion metadata\n",
    "df_bronze = (\n",
    "    df_bronze_raw\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"_source\", lit(source))\n",
    "    .withColumn(\"_status\", lit(\"valid\"))\n",
    ")\n",
    "\n",
    "# 3. Basic data-quality metrics\n",
    "df_bronze.show(n=4, truncate=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# count empty lines\n",
    "empty_records = df_bronze.filter(trim(col(\"_raw_data\")) == \"\").count()\n",
    "total_records = df_bronze.count()\n",
    "valid_records = total_records - empty_records # either it works or everything fails\n",
    "error_records = 0\n",
    "success_rates = (valid_records / total_records) * 100\n",
    "\n",
    "print(\"\\nData Quality Metrics:\")\n",
    "print(f\"total_records: {total_records}\")\n",
    "print(f\"valid_records: {valid_records}\")\n",
    "print(f\"empty_records: {empty_records}\")\n",
    "print(f\"error_records: {error_records}\")\n",
    "print(f\"success_rates: {success_rates}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74531f85-85a1-4076-8918-4309bda3983d",
   "metadata": {},
   "source": [
    "### Save data to bronze.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803433a8-6775-49b8-acba-64d06d09493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer saved to data/ukprop_bronze.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "output_path = \"data/ukprop_bronze.parquet\"\n",
    "df_bronze.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Bronze layer saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef65fdf-fc2a-4efd-a4dd-21dd1400f9df",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "+ RDD/Map-Reduce is technically superior for Bronze considering its core feature being low level fault tolerance and _status marking.\n",
    "+ Sure DFs is faster (0.10s compared to 4.5s), but less fault-tolerant for parsing errors. Hence RDD/Map-Reduce is better for Data Integrity, Resilience and auditability.\n",
    "+ Hence RDD/Map-Reduce is the most effective implementation for the bronze layer in that it that guarantees that all raw data is successfully cataloged, even the bad records. \n",
    "\n",
    "# END OF BRONZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f6816-7914-4773-a441-787181f4dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # stop spark notebook if necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
